{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUQMrOR9ficv"
      },
      "source": [
        "# Install and load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdSPyR-kGfHw",
        "outputId": "cfd856f6-60f4-4fd5-e051-a2b479385969"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets wandb accelerate kaleido psutil gputil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDWAQtgPNIry"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "import re, os\n",
        "from pathlib import Path\n",
        "import torch, wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification,Trainer,TrainingArguments, AutoConfig, EarlyStoppingCallback, IntervalStrategy\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, classification_report, f1_score, precision_recall_fscore_support\n",
        "from google.colab import drive\n",
        "import plotly.express as px\n",
        "import psutil\n",
        "import GPUtil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHmO5amTZC7"
      },
      "source": [
        "# Create variables and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlZm_27NeT7s"
      },
      "outputs": [],
      "source": [
        "NSAMPLE = 1 # 1 to run with the entire dataset\n",
        "RANDOM_SEED = 123\n",
        "TRESHOLD_PSEUDOLABELS = 0.6\n",
        "os.environ[\"WANDB_API_KEY\"] = ''\n",
        "pretrainedmodel = \"neuralmind/bert-large-portuguese-cased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C46JoAlNgRvn"
      },
      "outputs": [],
      "source": [
        "mydrive = Path('drive/MyDrive')\n",
        "data_folder = mydrive / 'thesis-data'\n",
        "bert_folder = mydrive / 'bertimbau-selftrain/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwdkg6-TN3W-"
      },
      "outputs": [],
      "source": [
        "def set_random_seed(seed=RANDOM_SEED):\n",
        "    \"\"\"\n",
        "    Function to set the random seed for numpy and PyTorch to ensure reproducibility.\n",
        "\n",
        "    Parameters:\n",
        "    seed (int): The seed value. Default is 42.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Set the seed for Numpy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set the seed for PyTorch\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # If you're running on GPU, you also need to set the seed for the GPU:\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def prepare_test_dataset(test, cols, tokenize_function):\n",
        "    \"\"\"\n",
        "    Function to prepare a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    test (DataFrame): The test DataFrame.\n",
        "    cols (list): List of column names to include in the test DataFrame.\n",
        "    tokenize_function (function): The function to tokenize the text data.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: The prepared test dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a copy of the required columns\n",
        "    test = test[cols].copy()\n",
        "\n",
        "    # Convert 'text' column to string type\n",
        "    test['text'] = test['text'].astype(str)\n",
        "\n",
        "    # Convert the DataFrame to a Dataset\n",
        "    test_dataset = Dataset.from_pandas(test)\n",
        "\n",
        "    # Apply the tokenization function to the 'text' column\n",
        "    test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return test\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=TOKEN_MAX_LEN)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "def predict(model,sentence, return_probabilities=False):\n",
        "    \"\"\"\n",
        "    Function to predict class label or probabilities for a given sentence.\n",
        "\n",
        "    Parameters:\n",
        "    sentence (str): Input sentence for prediction.\n",
        "    return_probabilities (bool): Flag to return probabilities. If False, the function\n",
        "                                 returns the label of the class with the highest score.\n",
        "                                 If True, the function returns the probabilities for each class.\n",
        "                                 Default is False.\n",
        "\n",
        "    Returns:\n",
        "    int or list: Predicted class label (if return_probabilities=False) or list of\n",
        "                 probabilities for each class (if return_probabilities=True).\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the sentence and prepare it for input to the model\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=TOKEN_MAX_LEN)\n",
        "\n",
        "    # Move input tensors to the same device as the model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Feed the inputs to the model and get the outputs\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Access the logits from the SequenceClassifierOutput object\n",
        "        logits = outputs.logits\n",
        "\n",
        "    if return_probabilities:\n",
        "        # Calculate the probabilities for each class\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
        "        return probabilities\n",
        "    else:\n",
        "        # Get the label of the class with the highest prediction score\n",
        "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "        return predicted_label\n",
        "\n",
        "def get_samples(df, column, label, N=10,threshold = TRESHOLD_PSEUDOLABELS):\n",
        "    \"\"\"\n",
        "    Extracts a specified number of samples (rows) from each quantile of a DataFrame based on a provided column and label.\n",
        "\n",
        "    This function assumes the input DataFrame contains a column with prediction confidence scores. It filters out rows\n",
        "    with scores below a certain threshold, and then categorizes the remaining rows into four quantiles based on these\n",
        "    scores. From each quantile, it randomly selects a set number of rows (samples) and returns them in a new DataFrame,\n",
        "    alongside the original text and a provided label.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The input DataFrame from which to extract samples.\n",
        "    column (str): The column in df that contains the prediction confidence scores.\n",
        "                  Rows with scores above a global 'threshold' are considered for sampling.\n",
        "    label (str): The label to assign to each of the extracted samples in the returned DataFrame.\n",
        "    N (int): The number of samples to extract from each quantile. If a quantile contains fewer than N samples,\n",
        "                       all samples from that quantile are included. Default is 10.\n",
        "    threshold(float): Minimum confidence needed to sample.\n",
        "\n",
        "    Returns:\n",
        "    samples_df (pd.DataFrame): A new DataFrame containing the extracted samples. This DataFrame has two columns:\n",
        "                               'text' - The original text from the input DataFrame\n",
        "                               'label' - The provided label\n",
        "\n",
        "    Notes:\n",
        "    This function uses pandas' qcut function to divide the 'column' into 4 equal quantiles.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Categorize the remaining rows into four quantiles based on the column\n",
        "    df['quantile'] = pd.qcut(df[column], q=4)\n",
        "\n",
        "    # Randomly sample N rows from each quantile\n",
        "    samples = df.groupby('quantile', group_keys=False).apply(lambda x: x.sample(N) if len(x) > N else x)\n",
        "\n",
        "    # Create a new DataFrame from the samples and assign the provided label to each sample\n",
        "    samples_df = pd.DataFrame(samples['text'])\n",
        "    samples_df['label'] = label\n",
        "\n",
        "    return samples_df\n",
        "\n",
        "\n",
        "def print_system_info():\n",
        "    print(f'Physical cores: {psutil.cpu_count(logical=False)}')\n",
        "    print(f'Total cores: {psutil.cpu_count(logical=True)}')\n",
        "    print(f'Memory: {psutil.virtual_memory().total / (1024.0 **3)}GB')\n",
        "\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    for i, gpu in enumerate(GPUs):\n",
        "        print(f'GPU {i}: {gpu.name} with {gpu.memoryTotal}MB memory')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw9P2uNPhrON"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    project_name,\n",
        "    run_name,\n",
        "    train_dataset=None,\n",
        "    eval_dataset=None,\n",
        "    pretrained_model=pretrainedmodel,\n",
        "    num_labels=2,\n",
        "    output_dir=bert_folder,\n",
        "    eval_steps=1_500,\n",
        "    warmup_steps=1_000,\n",
        "    learning_rate=2e-5,\n",
        "    dropout_rate=0.1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    seed=RANDOM_SEED,\n",
        "    compute_metrics=None,\n",
        "    patience=3):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    wandb.init(project=project_name, name=run_name, settings=wandb.Settings(start_method=\"thread\"))\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels=num_labels,\n",
        "                                                                hidden_dropout_prob=dropout_rate,\n",
        "                                                                attention_probs_dropout_prob=dropout_rate)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        save_steps=eval_steps,\n",
        "        warmup_steps=warmup_steps,\n",
        "        logging_steps=10,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        seed=seed,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"wandb\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        optim=\"adamw_torch\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'Training took {end_time - start_time} seconds')\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lousUoz2hrHK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(trainer, test_dataset):\n",
        "    \"\"\"\n",
        "    Function to evaluate a model on a test dataset and print relevant metrics.\n",
        "\n",
        "    Parameters:\n",
        "    trainer (Trainer): The Trainer object for the model.\n",
        "    test_dataset (Dataset): The test dataset.\n",
        "\n",
        "    Returns:\n",
        "    None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the predictions from the Trainer object\n",
        "    predictions = trainer.predict(test_dataset).predictions\n",
        "\n",
        "    # Get the true labels from the test dataset\n",
        "    true_labels = test_dataset['label']\n",
        "\n",
        "    # Calculate the predicted labels\n",
        "    predicted_labels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Generate and print the classification report\n",
        "    print(classification_report(true_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\"]))\n",
        "\n",
        "    # Calculate precision, recall, f1_score, and support\n",
        "    precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, predicted_labels)\n",
        "\n",
        "    # Print the F1-score for Class 1\n",
        "    class1_f1 = f1_score[1]  # Index 1 corresponds to Class 1\n",
        "    print(\"F1-score for Class 1:\", class1_f1)\n",
        "\n",
        "    # Print the mean F1-score\n",
        "    print(\"General F1:\", np.mean(f1_score))\n",
        "\n",
        "    # Print the mean recall\n",
        "    print(\"Recall:\", np.mean(recall))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXt8-8za_4ZZ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KchsQoiZTlVj",
        "outputId": "b98030ef-1177-4ffa-b866-f0b1c382674f"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "tqdm.pandas(desc=\"Running...\")\n",
        "\n",
        "set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxKT-VbPfWox"
      },
      "source": [
        "# Hardware diagnostic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkL9rttzgb9I",
        "outputId": "45041e59-8699-4981-e764-5ca8499481f1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnQT2PepfZH6",
        "outputId": "b201216f-9232-46b8-bdda-2dbbdb2c001e"
      },
      "outputs": [],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpDjh62Afdnp",
        "outputId": "35144baf-0631-48a8-c225-98c996a0fe5b"
      },
      "outputs": [],
      "source": [
        "print_system_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHTnMIrdtPF"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC5E0m2Sgfpi"
      },
      "outputs": [],
      "source": [
        "trainfile = data_folder / 'train' / 'train.csv'\n",
        "testfile = data_folder / 'test' / 'test_label.csv'\n",
        "testrawfile = data_folder / 'test' / 'test_raw.csv'\n",
        "unlabel_file = data_folder / 'train' / 'unlabel.csv'\n",
        "\n",
        "cols = ['text','label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sybN2VuHdvCK",
        "outputId": "c62d375c-1c2a-4c16-c669-1a310b973764"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(trainfile).sample(frac=NSAMPLE,random_state=RANDOM_SEED).set_index('id')\n",
        "test = pd.read_csv(testfile).sample(frac=NSAMPLE,random_state=RANDOM_SEED).set_index('id')\n",
        "testraw = pd.read_csv(testrawfile).sample(frac=NSAMPLE,random_state=RANDOM_SEED).set_index('id')\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(testraw.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZX7vcU5dnZD"
      },
      "source": [
        "# Define token lenght"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "069a871196014bdc9561a8694ff938d6"
          ]
        },
        "id": "yOZcSyHadrPv",
        "outputId": "5175f82c-8871-46c0-aeae-1959f0385e9b"
      },
      "outputs": [],
      "source": [
        "TOKEN_MAX_LEN = 512\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrainedmodel,do_lower_case=False)\n",
        "train_full_token = Dataset.from_pandas(train).map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "bCw7-xl3eDy4",
        "outputId": "f579c6bc-792c-4a1d-a00c-673b4d4e2904"
      },
      "outputs": [],
      "source": [
        "hist_series = pd.Series([len([v for v in seq if v != 0]) for seq in train_full_token['input_ids']])\n",
        "\n",
        "fig = px.histogram(hist_series, template='presentation')\n",
        "fig.update_layout(title='',\n",
        "                  xaxis_title='Token length',\n",
        "                  yaxis_title='Frequency',\n",
        "                  showlegend=False\n",
        "                  )\n",
        "\n",
        "fig.update_xaxes(showline=True,\n",
        "         linewidth=1,\n",
        "         linecolor='black',\n",
        "         mirror=True)\n",
        "\n",
        "fig.update_yaxes(showline=True,\n",
        "         linewidth=1,\n",
        "         linecolor='black',\n",
        "         mirror=True)\n",
        "fig.show()\n",
        "fig.write_image(\"tokenlen.png\", width=1200, height=800,scale=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az8xhswOfmI_"
      },
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "23741e3b475644a99b79da8bfa99c423"
          ]
        },
        "id": "LpB2b9wlGtO3",
        "outputId": "b41f613b-a679-4364-c293-dfc3d0b7664d"
      },
      "outputs": [],
      "source": [
        "TOKEN_MAX_LEN = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrainedmodel,do_lower_case=False)\n",
        "train_full_token = Dataset.from_pandas(train).map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "f9b489aa67384882bc10ce17ad7d26d7",
            "18a02d49e8b44b5390a1e7a0b3136e49",
            "96a8fbe55d57439f87935cd3188035a9"
          ]
        },
        "id": "x6ZY_1isI-MO",
        "outputId": "90cc4c29-d309-4c03-c69e-d03c47fb2877"
      },
      "outputs": [],
      "source": [
        "train = Dataset.from_pandas(train).map(tokenize_function, batched=True)\n",
        "\n",
        "test = prepare_test_dataset(test,cols,tokenize_function)\n",
        "testraw = prepare_test_dataset(testraw,cols,tokenize_function)\n",
        "\n",
        "train_split = train.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = train_split['train']\n",
        "val_dataset = train_split['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pqauz0Ufo7V"
      },
      "source": [
        "# Run model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6271ba2ed6694aad962eb8098d582ea6",
            "4a98717d55514324a8f0c0a3a52bbc6e",
            "14ec4f1543d64476a8319bc854c8193a",
            "523fb71682a14ba786fd44111cede99b",
            "8c99f5cad90248e0b4a9b4f9f5e81e8c",
            "18569354a96b463181cb844a76d2302b",
            "6bc33d23eac643ad8eb2ab99c321b98c",
            "3d41cb3d7d074752b0b3be8454c18e8f"
          ]
        },
        "id": "XZZJLlsLlrXg",
        "outputId": "6d2318d4-bd4b-42bf-cdee-cb748fdaab55"
      },
      "outputs": [],
      "source": [
        "trainer = train_model(\n",
        "    project_name=\"myproject\",\n",
        "    run_name=\"first-train\",\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "# 323 computers units -15h10\n",
        "# end 307 computer units- 18h16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53L_W_H6ftGr"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_W4O2_elO1y"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trainer, testraw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "OWjdovhhYChA",
        "outputId": "0b83cf7c-3752-4ab4-d948-26d092ee1d5d"
      },
      "outputs": [],
      "source": [
        "evaluate_model(trainer, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4uQyNEDdy8n"
      },
      "outputs": [],
      "source": [
        "sample_sentence = \"favela\"\n",
        "\n",
        "predicted_label = predict(trainer.model,sample_sentence)\n",
        "print(f\"Predicted label: {predicted_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LrbOxLNcklx"
      },
      "source": [
        "# Save first model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6rRmhOrcmOc",
        "outputId": "063c8a6d-5492-4227-c6ff-1d829537f05b"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(bert_folder)\n",
        "tokenizer.save_pretrained(bert_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwzx4UH5oVox"
      },
      "source": [
        "# Generate pseudolabels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "v5hrGOCTeE2v",
        "outputId": "d58a3539-8388-4285-97da-59f90bd0cbd9"
      },
      "outputs": [],
      "source": [
        "unlabel = pd.read_csv(unlabel_file).set_index('id')\n",
        "\n",
        "unlabel = unlabel.sample(frac=NSAMPLE)\n",
        "\n",
        "print(unlabel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "6Ydgsrevo_dZ",
        "outputId": "bf74280e-2790-43b9-f146-ad951b5be9c8"
      },
      "outputs": [],
      "source": [
        "unlabel['probas'] = unlabel['text'].progress_apply(lambda x: predict(trainer.model, x,return_probabilities=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "yKL-XpsSRVt8",
        "outputId": "2a010bce-b387-4ea8-c7ff-d7ff88bb629e"
      },
      "outputs": [],
      "source": [
        "pseudolabels = unlabel.join(pd.DataFrame(unlabel['probas'].to_list(),index=unlabel.index,columns=['class_0','class_1']))\n",
        "\n",
        "pseudolabels = pseudolabels[(pseudolabels.class_0 > TRESHOLD_PSEUDOLABELS) | (pseudolabels.class_1 > TRESHOLD_PSEUDOLABELS)].copy()\n",
        "\n",
        "pseudolabels['label'] = pseudolabels['probas'].apply(lambda x: np.argmax(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN-Aw_eTij6g"
      },
      "outputs": [],
      "source": [
        "pseudolabels.class_1.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtHgyxB_hGHS"
      },
      "outputs": [],
      "source": [
        "# Get samples for each class\n",
        "sample_neg = get_samples(pseudolabels, 'class_0', 0)\n",
        "sample_pos = get_samples(pseudolabels, 'class_1', 1)\n",
        "\n",
        "# Concatenate samples\n",
        "sample = pd.concat([sample_pos, sample_neg]).reset_index(drop=True)\n",
        "\n",
        "sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUpmOhv5UwF3"
      },
      "outputs": [],
      "source": [
        "sample.to_csv(data_folder / 'pseudolabels_to_validate.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5L_I8OehBD"
      },
      "source": [
        "# Merge dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwwIiCXEo3EY"
      },
      "outputs": [],
      "source": [
        "pseudolabels.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFazPkt-8RrR"
      },
      "outputs": [],
      "source": [
        "# Get boolean mask where each row is True if 'text' value is not in df['text']\n",
        "mask = ~pseudolabels['text'].isin(train['text'])\n",
        "\n",
        "# Use this mask to filter pseudolabels\n",
        "pseudolabels_filtered = pseudolabels[mask].drop_duplicates('text')\n",
        "\n",
        "# pseudolabels_filtered\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5zHp8RtrwGL"
      },
      "outputs": [],
      "source": [
        "pseudo_df = Dataset.from_pandas(pseudolabels_filtered[cols]).map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61lf9WUF8Ps9"
      },
      "outputs": [],
      "source": [
        "augmented_dataset = concatenate_datasets([train, pseudo_df]).train_test_split(test_size=0.1)\n",
        "train_dataset_2 = augmented_dataset['train']\n",
        "val_dataset_2 = augmented_dataset['test']\n",
        "\n",
        "train_dataset_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfgjwQnOrgur"
      },
      "outputs": [],
      "source": [
        "augmented_dataset.save_to_disk(data_folder/'augmented_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gbVXdKbuf2e"
      },
      "source": [
        "# Retrain and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDYev4xz3IZF"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "xQKrwQH5mRvG",
        "outputId": "11bee6e0-bbf9-4ce9-a738-5d5376c8c3b6"
      },
      "outputs": [],
      "source": [
        "trainer = train_model(\n",
        "    project_name=\"myproject\",\n",
        "    run_name=\"selftrain-retrain\",\n",
        "    train_dataset=train_dataset_2,\n",
        "    eval_dataset=val_dataset_2,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUqojMvinzj-"
      },
      "source": [
        "# Evaluate final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXf3oELimJJ8"
      },
      "outputs": [],
      "source": [
        "evaluate_model(self_trainer, testraw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCZsAPDrmKXh"
      },
      "outputs": [],
      "source": [
        "evaluate_model(self_trainer, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7pavrXun14s"
      },
      "source": [
        "# Export final model and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhwPEVY1n3v5"
      },
      "outputs": [],
      "source": [
        "self_trainer.model.save_pretrained(bert_folder/'final')\n",
        "tokenizer.save_pretrained(bert_folder/'final')\n",
        "augmented_dataset.save_to_disk(bert_folder/'final'/'data')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14ec4f1543d64476a8319bc854c8193a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bc33d23eac643ad8eb2ab99c321b98c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d41cb3d7d074752b0b3be8454c18e8f",
            "value": 1
          }
        },
        "18569354a96b463181cb844a76d2302b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d41cb3d7d074752b0b3be8454c18e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a98717d55514324a8f0c0a3a52bbc6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c99f5cad90248e0b4a9b4f9f5e81e8c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18569354a96b463181cb844a76d2302b",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "523fb71682a14ba786fd44111cede99b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6271ba2ed6694aad962eb8098d582ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a98717d55514324a8f0c0a3a52bbc6e",
              "IPY_MODEL_14ec4f1543d64476a8319bc854c8193a"
            ],
            "layout": "IPY_MODEL_523fb71682a14ba786fd44111cede99b"
          }
        },
        "6bc33d23eac643ad8eb2ab99c321b98c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c99f5cad90248e0b4a9b4f9f5e81e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
